{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9a8d4e",
   "metadata": {},
   "source": [
    "# Rule Based Classificiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "861fc8cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 312)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:312\u001b[1;36m\u001b[0m\n\u001b[1;33m    elif 0.3 < features['circularity'] < 0.8 and 9 <= features['average_hue'] < 32 and 0.6 < features['aspect_ratio'] < 1.1:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return np.array(image)\n",
    "\n",
    "def convolve(img, kernel):\n",
    "    k = kernel.shape[0] // 2\n",
    "    padded = np.pad(img, k, mode='edge')\n",
    "    output = np.zeros_like(img)\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            region = padded[i:i+2*k+1, j:j+2*k+1]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    return output\n",
    "\n",
    "# ===============   PREPROCESSING AND FILTERING   ===============\n",
    "\n",
    "def mean_filter(image, ksize=3):\n",
    "    kernel = np.ones((ksize, ksize)) / (ksize * ksize)\n",
    "    filtered = np.zeros_like(image)\n",
    "    for c in range(3):\n",
    "        filtered[:, :, c] = convolve(image[:, :, c], kernel)\n",
    "    return filtered\n",
    "\n",
    "def gaussian_filter(image, sigma=1):\n",
    "    ksize = int(2 * np.ceil(3 * sigma) + 1)\n",
    "    ax = np.linspace(-(ksize // 2), ksize // 2, ksize)\n",
    "    gauss = np.exp(-0.5 * np.square(ax) / sigma**2)\n",
    "    kernel = np.outer(gauss, gauss)\n",
    "    kernel /= np.sum(kernel)\n",
    "    if len(image.shape) == 2:\n",
    "        filtered = convolve(image, kernel)\n",
    "    else:\n",
    "        filtered = np.zeros_like(image)\n",
    "        for c in range(3):\n",
    "            filtered[:, :, c] = convolve(image[:, :, c], kernel)\n",
    "    return filtered\n",
    "\n",
    "def median_filter(image, ksize=3):\n",
    "    pad = ksize // 2\n",
    "    h, w, c = image.shape\n",
    "    padded = np.pad(image, ((pad, pad), (pad, pad), (0, 0)), mode='reflect')\n",
    "    filtered = np.zeros_like(image)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            for ch in range(c):\n",
    "                window = padded[i:i+ksize, j:j+ksize, ch]\n",
    "                filtered[i, j, ch] = np.median(window)\n",
    "    return filtered\n",
    "\n",
    "def adaptive_median_filter(image, max_window_size=7):\n",
    "    def process_channel(channel):\n",
    "        padded = np.pad(channel, ((max_window_size//2, ), (max_window_size//2, )), mode='reflect')\n",
    "        output = np.copy(channel)\n",
    "        for i in range(channel.shape[0]):\n",
    "            for j in range(channel.shape[1]):\n",
    "                for k in range(3, max_window_size + 1, 2):\n",
    "                    window = padded[i:i+k, j:j+k]\n",
    "                    z_min = np.min(window)\n",
    "                    z_max = np.max(window)\n",
    "                    z_med = np.median(window)\n",
    "                    z_xy = padded[i + max_window_size//2, j + max_window_size//2]\n",
    "                    if z_min < z_med < z_max:\n",
    "                        if z_min < z_xy < z_max:\n",
    "                            output[i, j] = z_xy\n",
    "                        else:\n",
    "                            output[i, j] = z_med\n",
    "                        break\n",
    "                    elif k == max_window_size:\n",
    "                        output[i, j] = z_med\n",
    "        return output\n",
    "    filtered = np.zeros_like(image)\n",
    "    for c in range(3):\n",
    "        filtered[:, :, c] = process_channel(image[:, :, c])\n",
    "    return filtered\n",
    "\n",
    "def unsharp_mask(image, sigma=1.0, amount=1.5):\n",
    "    blurred = gaussian_filter(image, sigma)\n",
    "    mask = image.astype(np.int32) - blurred.astype(np.int32)\n",
    "    sharpened = image + amount * mask\n",
    "    return np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "\n",
    "def preprocess_image(img_np):\n",
    "    img_np = mean_filter(img_np)\n",
    "    img_np = gaussian_filter(img_np, sigma=1)\n",
    "    img_np = median_filter(img_np)\n",
    "    img_np = adaptive_median_filter(img_np)\n",
    "    img_np = unsharp_mask(img_np, sigma=1.0, amount=1.5)\n",
    "    return img_np\n",
    "\n",
    "# ===============   COLOR SEGMENTATION   ===============\n",
    "def rgb_to_hsv(rgb_img):\n",
    "    rgb_norm = rgb_img.astype(np.float32) / 255.0\n",
    "    r, g, b = rgb_norm[:, :, 0], rgb_norm[:, :, 1], rgb_norm[:, :, 2]\n",
    "    v = np.max(rgb_norm, axis=2)\n",
    "    min_rgb = np.min(rgb_norm, axis=2)\n",
    "    delta = v - min_rgb\n",
    "    s = np.where(v == 0, 0, delta / v)\n",
    "    h = np.zeros_like(v)\n",
    "    red_max = (v == r) & (delta > 0)\n",
    "    green_max = (v == g) & (delta > 0)\n",
    "    blue_max = (v == b) & (delta > 0)\n",
    "    h[red_max] = ((g[red_max] - b[red_max]) / delta[red_max]) % 6\n",
    "    h[green_max] = ((b[green_max] - r[green_max]) / delta[green_max]) + 2\n",
    "    h[blue_max] = ((r[blue_max] - g[blue_max]) / delta[blue_max]) + 4\n",
    "    h = (h * 60) / 2\n",
    "    s = s * 255\n",
    "    v = v * 255\n",
    "    hsv_img = np.stack((h, s, v), axis=2).astype(np.uint8)\n",
    "    return hsv_img\n",
    "\n",
    "def segment_red_signs(hsv_img):\n",
    "    lower_red1 = np.array([0, 0.3 * 255, 0.2 * 255], dtype=np.uint8)\n",
    "    upper_red1 = np.array([15, 1.0 * 255, 1.0 * 255], dtype=np.uint8)\n",
    "    lower_red2 = np.array([165, 0.3 * 255, 0.2 * 255], dtype=np.uint8)\n",
    "    upper_red2 = np.array([180, 1.0 * 255, 1.0 * 255], dtype=np.uint8)\n",
    "    mask1 = cv2.inRange(hsv_img, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv_img, lower_red2, upper_red2)\n",
    "    red_mask = cv2.bitwise_or(mask1, mask2)\n",
    "    return red_mask\n",
    "\n",
    "def segment_blue_signs(hsv_img):\n",
    "    lower_blue = np.array([100, 0.3 * 255, 0.2 * 255], dtype=np.uint8)\n",
    "    upper_blue = np.array([130, 1.0 * 255, 1.0 * 255], dtype=np.uint8)\n",
    "    blue_mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "    return blue_mask\n",
    "\n",
    "def apply_morphological_operations(binary_mask, kernel_size=3):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    eroded = cv2.erode(binary_mask, kernel, iterations=1)\n",
    "    dilated = cv2.dilate(eroded, kernel, iterations=1)\n",
    "    return dilated\n",
    "\n",
    "def post_process_mask(binary_mask, kernel_size=3):\n",
    "    morphed = apply_morphological_operations(binary_mask, kernel_size)\n",
    "    return morphed\n",
    "\n",
    "def segment_traffic_signs(image):\n",
    "    hsv_img = rgb_to_hsv(image)\n",
    "    red_mask = segment_red_signs(hsv_img)\n",
    "    blue_mask = segment_blue_signs(hsv_img)\n",
    "    red_mask_processed = post_process_mask(red_mask)\n",
    "    blue_mask_processed = post_process_mask(blue_mask)\n",
    "    combined_mask = np.bitwise_or(red_mask_processed, blue_mask_processed)\n",
    "    return combined_mask\n",
    "\n",
    "# ===============   EDGE DETECTION   ===============\n",
    "\n",
    "def sobel_filters(img):\n",
    "    Kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "    Ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    Gx = convolve(img.astype(np.float32), Kx)\n",
    "    Gy = convolve(img.astype(np.float32), Ky)\n",
    "    magnitude = np.hypot(Gx, Gy).astype(np.uint8)\n",
    "    angle = np.arctan2(Gy, Gx) * 180 / np.pi\n",
    "    angle = (angle + 180) % 180\n",
    "    return magnitude, angle\n",
    "\n",
    "def non_maximum_suppression(magnitude, angle):\n",
    "    Z = np.zeros(magnitude.shape)\n",
    "    angle = angle // 45 * 45\n",
    "    for i in range(1, magnitude.shape[0]-1):\n",
    "        for j in range(1, magnitude.shape[1]-1):\n",
    "            q = r = 255\n",
    "            if (0 <= angle[i, j] < 22.5) or (157.5 <= angle[i, j] <= 180):\n",
    "                q = magnitude[i, j+1]\n",
    "                r = magnitude[i, j-1]\n",
    "            elif (22.5 <= angle[i, j] < 67.5):\n",
    "                q = magnitude[i+1, j-1]\n",
    "                r = magnitude[i-1, j+1]\n",
    "            elif (67.5 <= angle[i, j] < 112.5):\n",
    "                q = magnitude[i+1, j]\n",
    "                r = magnitude[i-1, j]\n",
    "            elif (112.5 <= angle[i, j] < 157.5):\n",
    "                q = magnitude[i-1, j-1]\n",
    "                r = magnitude[i+1, j+1]\n",
    "            if (magnitude[i, j] >= q) and (magnitude[i, j] >= r):\n",
    "                Z[i, j] = magnitude[i, j]\n",
    "            else:\n",
    "                Z[i, j] = 0\n",
    "    return Z\n",
    "\n",
    "def double_thresholding(img, low_ratio=0.05, high_ratio=0.15):\n",
    "    high_threshold = img.max() * high_ratio\n",
    "    low_threshold = high_threshold * low_ratio\n",
    "    res = np.zeros(img.shape)\n",
    "    weak = np.uint8(75)\n",
    "    strong = np.uint8(255)\n",
    "    strong_i, strong_j = np.where(img >= high_threshold)\n",
    "    weak_i, weak_j = np.where((img <= high_threshold) & (img >= low_threshold))\n",
    "    res[strong_i, strong_j] = strong\n",
    "    res[weak_i, weak_j] = weak\n",
    "    return res, weak, strong\n",
    "\n",
    "def edge_tracking(img, weak, strong=255):\n",
    "    h, w = img.shape\n",
    "    for i in range(1, h-1):\n",
    "        for j in range(1, w-1):\n",
    "            if img[i, j] == weak:\n",
    "                if ((img[i+1, j-1] == strong) or (img[i+1, j] == strong) or (img[i+1, j+1] == strong)\n",
    "                    or (img[i, j-1] == strong) or (img[i, j+1] == strong)\n",
    "                    or (img[i-1, j-1] == strong) or (img[i-1, j] == strong) or (img[i-1, j+1] == strong)):\n",
    "                    img[i, j] = strong\n",
    "                else:\n",
    "                    img[i, j] = 0\n",
    "    return img\n",
    "\n",
    "def canny_edge(img):\n",
    "    blurred = gaussian_filter(img)\n",
    "    magnitude, angle = sobel_filters(blurred)\n",
    "    non_max = non_maximum_suppression(magnitude, angle)\n",
    "    thresholded, weak, strong = double_thresholding(non_max)\n",
    "    final_edges = edge_tracking(thresholded, weak, strong)\n",
    "    return final_edges\n",
    "\n",
    "# =============== GEOMETRIC NORMALIZATION   ===============\n",
    "\n",
    "def get_upright_rotation_angle(binary_mask):\n",
    "    contours, _ = cv2.findContours(binary_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        rect = cv2.minAreaRect(contours[0])\n",
    "        angle = rect[-1]\n",
    "        if angle < -45:\n",
    "            angle += 90\n",
    "        return -angle\n",
    "    return 0\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def resize_image(img, target_size=(200, 200)):\n",
    "    return cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def geometric_normalization(binary_mask):\n",
    "    angle = get_upright_rotation_angle(binary_mask)\n",
    "    rotated_mask = rotate_image(binary_mask, angle)\n",
    "    resized_mask = resize_image(rotated_mask, (200, 200))\n",
    "    return resized_mask\n",
    "\n",
    "# ===============   FEATURE EXTRACTION   ===============\n",
    "\n",
    "def extract_corner_count(binary_mask):\n",
    "    gray = (binary_mask * 255).astype(np.uint8)\n",
    "    dst = cv2.cornerHarris(gray, blockSize=2, ksize=3, k=0.04)\n",
    "    corners = np.where(dst > 0.01 * dst.max())\n",
    "    return len(corners[0])\n",
    "\n",
    "def calculate_circularity(binary_mask):\n",
    "    contours, _ = cv2.findContours(binary_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        area = cv2.contourArea(contours[0])\n",
    "        perimeter = cv2.arcLength(contours[0], True)\n",
    "        if perimeter == 0:\n",
    "            return 0\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2)\n",
    "        return circularity\n",
    "    return 0\n",
    "\n",
    "def calculate_aspect_ratio_extent(binary_mask):\n",
    "    contours, _ = cv2.findContours(binary_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        x, y, w, h = cv2.boundingRect(contours[0])\n",
    "        area = cv2.contourArea(contours[0])\n",
    "        if w == 0 or h == 0:\n",
    "            return 0, 0\n",
    "        aspect_ratio = w / h\n",
    "        extent = area / (w * h)\n",
    "        return aspect_ratio, extent\n",
    "    return 0, 0\n",
    "\n",
    "def calculate_average_hue(hsv_img, binary_mask):\n",
    "    hue_values = hsv_img[:, :, 0][binary_mask > 0]\n",
    "    if hue_values.size > 0:\n",
    "        return np.mean(hue_values)\n",
    "    return 0\n",
    "\n",
    "def extract_features(normalized_mask, original_hsv_img, original_binary_mask):\n",
    "    corner_count = extract_corner_count(normalized_mask)\n",
    "    circularity = calculate_circularity(normalized_mask)\n",
    "    aspect_ratio, extent = calculate_aspect_ratio_extent(normalized_mask)\n",
    "    average_hue = calculate_average_hue(original_hsv_img, original_binary_mask)\n",
    "    return {\n",
    "        'corner_count': corner_count,\n",
    "        'circularity': circularity,\n",
    "        'aspect_ratio': aspect_ratio,\n",
    "        'extent': extent,\n",
    "        'average_hue': average_hue\n",
    "    }\n",
    "\n",
    "# =============== RULE BASED CLASSIFICATION ================\n",
    "\n",
    "def classify_traffic_sign(features):\n",
    "    \"\"\"Applies rules to classify the traffic sign based on extracted features.\"\"\"\n",
    "    predicted_label = None\n",
    "    if 0.2 < features['circularity'] < 0.8 and 0.5 < features['aspect_ratio'] < 1.6 and 0.5 < features['extent'] < 0.9 and 77 < features['average_hue'] < 102:\n",
    "        predicted_label = 14\n",
    "    elif 0 <= features['circularity']  < 0.9 and 0 <= features['average_hue'] < 83 and 0 <= features['aspect_ratio'] < 1.5 and 0 <= features['extent'] < 1.0:\n",
    "        predicted_label = 1\n",
    "    elif 0.3 < features['circularity'] < 0.8 and 9 <= features['average_hue'] < 32 and 0.6 < features['aspect_ratio'] < 1.1:\n",
    "        predicted_label = 28\n",
    "\n",
    "    # Rule for No Entry (Class ID 17): Circular with a central horizontal bar (might be captured by aspect ratio/extent) and red\n",
    "    if features['circularity'] < 0.8 and 0 <= features['average_hue'] <= 136 and 0 <= features['aspect_ratio'] < 3.1 and 0 <= features['extent'] < 1.0:\n",
    "        predicted_label = 17\n",
    "\n",
    "    # Rule for Priority Road (Class ID 12): Diamond shape (lower circularity, specific aspect ratio/extent) and yellow/white\n",
    "    if 0. <= features['circularity'] < 0.8 and 0 <= features['aspect_ratio'] < 2.2 and 0 <= features['extent'] < 1.0 and 0 <= features['average_hue'] < 113: # Assuming yellow hue range\n",
    "        predicted_label = 12\n",
    "\n",
    "    # Rule for Speed Limit 20 (Class ID 0): Generally circular and potentially red\n",
    "    if 0.9 > features['circularity'] >= 0 and 31 < features['average_hue'] < 114 and 0.4 < features['extent'] < 1.0 and 0.3 <= features['aspect_ratio'] < 1.7:\n",
    "        predicted_label = 0\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# ===============   EVALUATION FUNCTIONS   ===============\n",
    "\n",
    "def evaluate_performance(results_df):\n",
    "    \"\"\"Calculates overall accuracy and class-wise metrics.\"\"\"\n",
    "    correct_predictions = results_df['correct'].sum()\n",
    "    total_predictions = len(results_df)\n",
    "    overall_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    class_metrics = {}\n",
    "    for class_id in results_df['ground_truth'].unique():\n",
    "        class_data = results_df[results_df['ground_truth'] == class_id]\n",
    "        true_positives = class_data[class_data['ground_truth'] == class_data['predicted']].shape[0]\n",
    "        false_positives = results_df[(results_df['ground_truth'] != class_id) & (results_df['predicted'] == class_id)].shape[0]\n",
    "        false_negatives = results_df[(results_df['ground_truth'] == class_id) & (results_df['predicted'] != class_id)].shape[0]\n",
    "        total_class = class_data.shape[0]\n",
    "        class_accuracy = true_positives / total_class if total_class > 0 else 0\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        class_metrics[class_id] = {'precision': precision, 'recall': recall, 'accuracy': class_accuracy}\n",
    "    return overall_accuracy, class_metrics\n",
    "\n",
    "def generate_confusion_matrix(results_df, class_labels):\n",
    "    \"\"\"Generates and saves the confusion matrix heatmap.\"\"\"\n",
    "    cm = pd.crosstab(results_df['ground_truth'], results_df['predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(CONFUSION_MATRIX_FILE)\n",
    "    plt.close()\n",
    "\n",
    "def write_metrics(overall_accuracy, class_metrics):\n",
    "    \"\"\"Writes the overall accuracy and class-wise metrics to a text file.\"\"\"\n",
    "    with open(METRICS_FILE, 'w') as f:\n",
    "        f.write(f\"Overall Accuracy: {overall_accuracy:.4f}\\n\\n\")\n",
    "        f.write(\"Class-wise Metrics:\\n\")\n",
    "        for class_id, metrics in class_metrics.items():\n",
    "            f.write(f\"Class {class_id}:\\n\")\n",
    "            f.write(f\"  Precision: {metrics['precision']:.4f}\\n\")\n",
    "            f.write(f\"  Recall: {metrics['recall']:.4f}\\n\")\n",
    "            f.write(f\"  Accuracy: {metrics['accuracy']:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def write_results_csv(results_df):\n",
    "    \"\"\"Writes the results DataFrame to a CSV file.\"\"\"\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "\n",
    "# =============== MAIN EXECUTION ===============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Project Setup ---\n",
    "    PROJECT_ROOT = \".\"\n",
    "    CODE_DIR = os.path.join(PROJECT_ROOT, \"code\")\n",
    "    RESULTS_FILE = os.path.join(PROJECT_ROOT, \"results.csv\")\n",
    "    METRICS_FILE = os.path.join(PROJECT_ROOT, \"metrics.txt\")\n",
    "    CONFUSION_MATRIX_FILE = os.path.join(PROJECT_ROOT, \"confusion_matrix.png\")\n",
    "    DATA_DIR = os.path.join(PROJECT_ROOT, \"./\")\n",
    "    os.makedirs(CODE_DIR, exist_ok=True)\n",
    "\n",
    "    # 1. Select 6 to 8 different traffic sign classes\n",
    "    selected_classes = [17,0, 1, 12, 14, 28]\n",
    "    class_label_mapping = {0: 'speed limit 20', 1: 'speed limit 30', 28: 'school crossing', 12: 'priority road', 14: 'stop', 17: 'no entry'}\n",
    "    selected_class_labels = [class_label_mapping[i] for i in selected_classes]\n",
    "\n",
    "    results = []\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    train_subset_df = train_df[train_df['ClassId'].isin(selected_classes)].head(100 * len(selected_classes)).reset_index(drop=True)\n",
    "\n",
    "    for index, row in train_subset_df.iterrows():\n",
    "        image_path = os.path.join(DATA_DIR, row['Path'])\n",
    "        ground_truth_label = row['ClassId']\n",
    "        filename = os.path.basename(image_path)\n",
    "\n",
    "        try:\n",
    "            image = load_image(image_path)\n",
    "            preprocessed_image = preprocess_image(image.copy())\n",
    "            segmented_mask = segment_traffic_signs(preprocessed_image.copy())\n",
    "            normalized_mask = geometric_normalization(segmented_mask.copy())\n",
    "            hsv_image = rgb_to_hsv(preprocessed_image.copy())\n",
    "            features = extract_features(normalized_mask.copy(), hsv_image.copy(), segmented_mask.copy())\n",
    "            predicted_label = classify_traffic_sign(features)\n",
    "            correct = 1 if predicted_label == ground_truth_label else 0\n",
    "            results.append([filename, ground_truth_label, predicted_label, correct])\n",
    "            print(f\"Features: {features}\")\n",
    "            print(f\"Processed: {filename} - Ground Truth: {ground_truth_label}, Predicted: {predicted_label}, Correct: {correct}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "    results_df = pd.DataFrame(results, columns=['filename', 'ground_truth', 'predicted', 'correct'])\n",
    "    overall_accuracy, class_metrics = evaluate_performance(results_df)\n",
    "    generate_confusion_matrix(results_df, selected_class_labels)\n",
    "    write_metrics(overall_accuracy, class_metrics)\n",
    "    write_results_csv(results_df)\n",
    "    print(\"Evaluation complete. Results, metrics, and confusion matrix saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c6fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d114d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2622697b",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc2d9c4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding image files...\n",
      "Found 43 classes: 0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 4, 40, 41, 42, 5, 6, 7, 8, 9\n",
      "Found 39209 image files across 43 classes\n",
      "Processing images and extracting features...\n",
      "Processed batch 1/785\n",
      "Processed batch 2/785\n",
      "Processed batch 3/785\n",
      "Processed batch 4/785\n",
      "Processed batch 5/785\n",
      "Processed batch 6/785\n",
      "Processed batch 7/785\n",
      "Processed batch 8/785\n",
      "Processed batch 9/785\n",
      "Processed batch 10/785\n",
      "Processed batch 11/785\n",
      "Processed batch 12/785\n",
      "Processed batch 13/785\n",
      "Processed batch 14/785\n",
      "Processed batch 15/785\n",
      "Processed batch 16/785\n",
      "Processed batch 17/785\n",
      "Processed batch 18/785\n",
      "Processed batch 19/785\n",
      "Processed batch 20/785\n",
      "Processed batch 21/785\n",
      "Processed batch 22/785\n",
      "Processed batch 23/785\n",
      "Processed batch 24/785\n",
      "Processed batch 25/785\n",
      "Processed batch 26/785\n",
      "Processed batch 27/785\n",
      "Processed batch 28/785\n",
      "Processed batch 29/785\n",
      "Processed batch 30/785\n",
      "Processed batch 31/785\n",
      "Processed batch 32/785\n",
      "Processed batch 33/785\n",
      "Processed batch 34/785\n",
      "Processed batch 35/785\n",
      "Processed batch 36/785\n",
      "Processed batch 37/785\n",
      "Processed batch 38/785\n",
      "Processed batch 39/785\n",
      "Processed batch 40/785\n",
      "Processed batch 41/785\n",
      "Processed batch 42/785\n",
      "Processed batch 43/785\n",
      "Processed batch 44/785\n",
      "Processed batch 45/785\n",
      "Processed batch 46/785\n",
      "Processed batch 47/785\n",
      "Processed batch 48/785\n",
      "Processed batch 49/785\n",
      "Processed batch 50/785\n",
      "Processed batch 51/785\n",
      "Processed batch 52/785\n",
      "Processed batch 53/785\n",
      "Processed batch 54/785\n",
      "Processed batch 55/785\n",
      "Processed batch 56/785\n",
      "Processed batch 57/785\n",
      "Processed batch 58/785\n",
      "Processed batch 59/785\n",
      "Processed batch 60/785\n",
      "Processed batch 61/785\n",
      "Processed batch 62/785\n",
      "Processed batch 63/785\n",
      "Processed batch 64/785\n",
      "Processed batch 65/785\n",
      "Processed batch 66/785\n",
      "Processed batch 67/785\n",
      "Processed batch 68/785\n",
      "Processed batch 69/785\n",
      "Processed batch 70/785\n",
      "Processed batch 71/785\n",
      "Processed batch 72/785\n",
      "Processed batch 73/785\n",
      "Processed batch 74/785\n",
      "Processed batch 75/785\n",
      "Processed batch 76/785\n",
      "Processed batch 77/785\n",
      "Processed batch 78/785\n",
      "Processed batch 79/785\n",
      "Processed batch 80/785\n",
      "Processed batch 81/785\n",
      "Processed batch 82/785\n",
      "Processed batch 83/785\n",
      "Processed batch 84/785\n",
      "Processed batch 85/785\n",
      "Processed batch 86/785\n",
      "Processed batch 87/785\n",
      "Processed batch 88/785\n",
      "Processed batch 89/785\n",
      "Processed batch 90/785\n",
      "Processed batch 91/785\n",
      "Processed batch 92/785\n",
      "Processed batch 93/785\n",
      "Processed batch 94/785\n",
      "Processed batch 95/785\n",
      "Processed batch 96/785\n",
      "Processed batch 97/785\n",
      "Processed batch 98/785\n",
      "Processed batch 99/785\n",
      "Processed batch 100/785\n",
      "Processed batch 101/785\n",
      "Processed batch 102/785\n",
      "Processed batch 103/785\n",
      "Processed batch 104/785\n",
      "Processed batch 105/785\n",
      "Processed batch 106/785\n",
      "Processed batch 107/785\n",
      "Processed batch 108/785\n",
      "Processed batch 109/785\n",
      "Processed batch 110/785\n",
      "Processed batch 111/785\n",
      "Processed batch 112/785\n",
      "Processed batch 113/785\n",
      "Processed batch 114/785\n",
      "Processed batch 115/785\n",
      "Processed batch 116/785\n",
      "Processed batch 117/785\n",
      "Processed batch 118/785\n",
      "Processed batch 119/785\n",
      "Processed batch 120/785\n",
      "Processed batch 121/785\n",
      "Processed batch 122/785\n",
      "Processed batch 123/785\n",
      "Processed batch 124/785\n",
      "Processed batch 125/785\n",
      "Processed batch 126/785\n",
      "Processed batch 127/785\n",
      "Processed batch 128/785\n",
      "Processed batch 129/785\n",
      "Processed batch 130/785\n",
      "Processed batch 131/785\n",
      "Processed batch 132/785\n",
      "Processed batch 133/785\n",
      "Processed batch 134/785\n",
      "Processed batch 135/785\n",
      "Processed batch 136/785\n",
      "Processed batch 137/785\n",
      "Processed batch 138/785\n",
      "Processed batch 139/785\n",
      "Processed batch 140/785\n",
      "Processed batch 141/785\n",
      "Processed batch 142/785\n",
      "Processed batch 143/785\n",
      "Processed batch 144/785\n",
      "Processed batch 145/785\n",
      "Processed batch 146/785\n",
      "Processed batch 147/785\n",
      "Processed batch 148/785\n",
      "Processed batch 149/785\n",
      "Processed batch 150/785\n",
      "Processed batch 151/785\n",
      "Processed batch 152/785\n",
      "Processed batch 153/785\n",
      "Processed batch 154/785\n",
      "Processed batch 155/785\n",
      "Processed batch 156/785\n",
      "Processed batch 157/785\n",
      "Processed batch 158/785\n",
      "Processed batch 159/785\n",
      "Processed batch 160/785\n",
      "Processed batch 161/785\n",
      "Processed batch 162/785\n",
      "Processed batch 163/785\n",
      "Processed batch 164/785\n",
      "Processed batch 165/785\n",
      "Processed batch 166/785\n",
      "Processed batch 167/785\n",
      "Processed batch 168/785\n",
      "Processed batch 169/785\n",
      "Processed batch 170/785\n",
      "Processed batch 171/785\n",
      "Processed batch 172/785\n",
      "Processed batch 173/785\n",
      "Processed batch 174/785\n",
      "Processed batch 175/785\n",
      "Processed batch 176/785\n",
      "Processed batch 177/785\n",
      "Processed batch 178/785\n",
      "Processed batch 179/785\n",
      "Processed batch 180/785\n",
      "Processed batch 181/785\n",
      "Processed batch 182/785\n",
      "Processed batch 183/785\n",
      "Processed batch 184/785\n",
      "Processed batch 185/785\n",
      "Processed batch 186/785\n",
      "Processed batch 187/785\n",
      "Processed batch 188/785\n",
      "Processed batch 189/785\n",
      "Processed batch 190/785\n",
      "Processed batch 191/785\n",
      "Processed batch 192/785\n",
      "Processed batch 193/785\n",
      "Processed batch 194/785\n",
      "Processed batch 195/785\n",
      "Processed batch 196/785\n",
      "Processed batch 197/785\n",
      "Processed batch 198/785\n",
      "Processed batch 199/785\n",
      "Processed batch 200/785\n",
      "Processed batch 201/785\n",
      "Processed batch 202/785\n",
      "Processed batch 203/785\n",
      "Processed batch 204/785\n",
      "Processed batch 205/785\n",
      "Processed batch 206/785\n",
      "Processed batch 207/785\n",
      "Processed batch 208/785\n",
      "Processed batch 209/785\n",
      "Processed batch 210/785\n",
      "Processed batch 211/785\n",
      "Processed batch 212/785\n",
      "Processed batch 213/785\n",
      "Processed batch 214/785\n",
      "Processed batch 215/785\n",
      "Processed batch 216/785\n",
      "Processed batch 217/785\n",
      "Processed batch 218/785\n",
      "Processed batch 219/785\n",
      "Processed batch 220/785\n",
      "Processed batch 221/785\n",
      "Processed batch 222/785\n",
      "Processed batch 223/785\n",
      "Processed batch 224/785\n",
      "Processed batch 225/785\n",
      "Processed batch 226/785\n",
      "Processed batch 227/785\n",
      "Processed batch 228/785\n",
      "Processed batch 229/785\n",
      "Processed batch 230/785\n",
      "Processed batch 231/785\n",
      "Processed batch 232/785\n",
      "Processed batch 233/785\n",
      "Processed batch 234/785\n",
      "Processed batch 235/785\n",
      "Processed batch 236/785\n",
      "Processed batch 237/785\n",
      "Processed batch 238/785\n",
      "Processed batch 239/785\n",
      "Processed batch 240/785\n",
      "Processed batch 241/785\n",
      "Processed batch 242/785\n",
      "Processed batch 243/785\n",
      "Processed batch 244/785\n",
      "Processed batch 245/785\n",
      "Processed batch 246/785\n",
      "Processed batch 247/785\n",
      "Processed batch 248/785\n",
      "Processed batch 249/785\n",
      "Processed batch 250/785\n",
      "Processed batch 251/785\n",
      "Processed batch 252/785\n",
      "Processed batch 253/785\n",
      "Processed batch 254/785\n",
      "Processed batch 255/785\n",
      "Processed batch 256/785\n",
      "Processed batch 257/785\n",
      "Processed batch 258/785\n",
      "Processed batch 259/785\n",
      "Processed batch 260/785\n",
      "Processed batch 261/785\n",
      "Processed batch 262/785\n",
      "Processed batch 263/785\n",
      "Processed batch 264/785\n",
      "Processed batch 265/785\n",
      "Processed batch 266/785\n",
      "Processed batch 267/785\n",
      "Processed batch 268/785\n",
      "Processed batch 269/785\n",
      "Processed batch 270/785\n",
      "Processed batch 271/785\n",
      "Processed batch 272/785\n",
      "Processed batch 273/785\n",
      "Processed batch 274/785\n",
      "Processed batch 275/785\n",
      "Processed batch 276/785\n",
      "Processed batch 277/785\n",
      "Processed batch 278/785\n",
      "Processed batch 279/785\n",
      "Processed batch 280/785\n",
      "Processed batch 281/785\n",
      "Processed batch 282/785\n",
      "Processed batch 283/785\n",
      "Processed batch 284/785\n",
      "Processed batch 285/785\n",
      "Processed batch 286/785\n",
      "Processed batch 287/785\n",
      "Processed batch 288/785\n",
      "Processed batch 289/785\n",
      "Processed batch 290/785\n",
      "Processed batch 291/785\n",
      "Processed batch 292/785\n",
      "Processed batch 293/785\n",
      "Processed batch 294/785\n",
      "Processed batch 295/785\n",
      "Processed batch 296/785\n",
      "Processed batch 297/785\n",
      "Processed batch 298/785\n",
      "Processed batch 299/785\n",
      "Processed batch 300/785\n",
      "Processed batch 301/785\n",
      "Processed batch 302/785\n",
      "Processed batch 303/785\n",
      "Processed batch 304/785\n",
      "Processed batch 305/785\n",
      "Processed batch 306/785\n",
      "Processed batch 307/785\n",
      "Processed batch 308/785\n",
      "Processed batch 309/785\n",
      "Processed batch 310/785\n",
      "Processed batch 311/785\n",
      "Processed batch 312/785\n",
      "Processed batch 313/785\n",
      "Processed batch 314/785\n",
      "Processed batch 315/785\n",
      "Processed batch 316/785\n",
      "Processed batch 317/785\n",
      "Processed batch 318/785\n",
      "Processed batch 319/785\n",
      "Processed batch 320/785\n",
      "Processed batch 321/785\n",
      "Processed batch 322/785\n",
      "Processed batch 323/785\n",
      "Processed batch 324/785\n",
      "Processed batch 325/785\n",
      "Processed batch 326/785\n",
      "Processed batch 327/785\n",
      "Processed batch 328/785\n",
      "Processed batch 329/785\n",
      "Processed batch 330/785\n",
      "Processed batch 331/785\n",
      "Processed batch 332/785\n",
      "Processed batch 333/785\n",
      "Processed batch 334/785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 335/785\n",
      "Processed batch 336/785\n",
      "Processed batch 337/785\n",
      "Processed batch 338/785\n",
      "Processed batch 339/785\n",
      "Processed batch 340/785\n",
      "Processed batch 341/785\n",
      "Processed batch 342/785\n",
      "Processed batch 343/785\n",
      "Processed batch 344/785\n",
      "Processed batch 345/785\n",
      "Processed batch 346/785\n",
      "Processed batch 347/785\n",
      "Processed batch 348/785\n",
      "Processed batch 349/785\n",
      "Processed batch 350/785\n",
      "Processed batch 351/785\n",
      "Processed batch 352/785\n",
      "Processed batch 353/785\n",
      "Processed batch 354/785\n",
      "Processed batch 355/785\n",
      "Processed batch 356/785\n",
      "Processed batch 357/785\n",
      "Processed batch 358/785\n",
      "Processed batch 359/785\n",
      "Processed batch 360/785\n",
      "Processed batch 361/785\n",
      "Processed batch 362/785\n",
      "Processed batch 363/785\n",
      "Processed batch 364/785\n",
      "Processed batch 365/785\n",
      "Processed batch 366/785\n",
      "Processed batch 367/785\n",
      "Processed batch 368/785\n",
      "Processed batch 369/785\n",
      "Processed batch 370/785\n",
      "Processed batch 371/785\n",
      "Processed batch 372/785\n",
      "Processed batch 373/785\n",
      "Processed batch 374/785\n",
      "Processed batch 375/785\n",
      "Processed batch 376/785\n",
      "Processed batch 377/785\n",
      "Processed batch 378/785\n",
      "Processed batch 379/785\n",
      "Processed batch 380/785\n",
      "Processed batch 381/785\n",
      "Processed batch 382/785\n",
      "Processed batch 383/785\n",
      "Processed batch 384/785\n",
      "Processed batch 385/785\n",
      "Processed batch 386/785\n",
      "Processed batch 387/785\n",
      "Processed batch 388/785\n",
      "Processed batch 389/785\n",
      "Processed batch 390/785\n",
      "Processed batch 391/785\n",
      "Processed batch 392/785\n",
      "Processed batch 393/785\n",
      "Processed batch 394/785\n",
      "Processed batch 395/785\n",
      "Processed batch 396/785\n",
      "Processed batch 397/785\n",
      "Processed batch 398/785\n",
      "Processed batch 399/785\n",
      "Processed batch 400/785\n",
      "Processed batch 401/785\n",
      "Processed batch 402/785\n",
      "Processed batch 403/785\n",
      "Processed batch 404/785\n",
      "Processed batch 405/785\n",
      "Processed batch 406/785\n",
      "Processed batch 407/785\n",
      "Processed batch 408/785\n",
      "Processed batch 409/785\n",
      "Processed batch 410/785\n",
      "Processed batch 411/785\n",
      "Processed batch 412/785\n",
      "Processed batch 413/785\n",
      "Processed batch 414/785\n",
      "Processed batch 415/785\n",
      "Processed batch 416/785\n",
      "Processed batch 417/785\n",
      "Processed batch 418/785\n",
      "Processed batch 419/785\n",
      "Processed batch 420/785\n",
      "Processed batch 421/785\n",
      "Processed batch 422/785\n",
      "Processed batch 423/785\n",
      "Processed batch 424/785\n",
      "Processed batch 425/785\n",
      "Processed batch 426/785\n",
      "Processed batch 427/785\n",
      "Processed batch 428/785\n",
      "Processed batch 429/785\n",
      "Processed batch 430/785\n",
      "Processed batch 431/785\n",
      "Processed batch 432/785\n",
      "Processed batch 433/785\n",
      "Processed batch 434/785\n",
      "Processed batch 435/785\n",
      "Processed batch 436/785\n",
      "Processed batch 437/785\n",
      "Processed batch 438/785\n",
      "Processed batch 439/785\n",
      "Processed batch 440/785\n",
      "Processed batch 441/785\n",
      "Processed batch 442/785\n",
      "Processed batch 443/785\n",
      "Processed batch 444/785\n",
      "Processed batch 445/785\n",
      "Processed batch 446/785\n",
      "Processed batch 447/785\n",
      "Processed batch 448/785\n",
      "Processed batch 449/785\n",
      "Processed batch 450/785\n",
      "Processed batch 451/785\n",
      "Processed batch 452/785\n",
      "Processed batch 453/785\n",
      "Processed batch 454/785\n",
      "Processed batch 455/785\n",
      "Processed batch 456/785\n",
      "Processed batch 457/785\n",
      "Processed batch 458/785\n",
      "Processed batch 459/785\n",
      "Processed batch 460/785\n",
      "Processed batch 461/785\n",
      "Processed batch 462/785\n",
      "Processed batch 463/785\n",
      "Processed batch 464/785\n",
      "Processed batch 465/785\n",
      "Processed batch 466/785\n",
      "Processed batch 467/785\n",
      "Processed batch 468/785\n",
      "Processed batch 469/785\n",
      "Processed batch 470/785\n",
      "Processed batch 471/785\n",
      "Processed batch 472/785\n",
      "Processed batch 473/785\n",
      "Processed batch 474/785\n",
      "Processed batch 475/785\n",
      "Processed batch 476/785\n",
      "Processed batch 477/785\n",
      "Processed batch 478/785\n",
      "Processed batch 479/785\n",
      "Processed batch 480/785\n",
      "Processed batch 481/785\n",
      "Processed batch 482/785\n",
      "Processed batch 483/785\n",
      "Processed batch 484/785\n",
      "Processed batch 485/785\n",
      "Processed batch 486/785\n",
      "Processed batch 487/785\n",
      "Processed batch 488/785\n",
      "Processed batch 489/785\n",
      "Processed batch 490/785\n",
      "Processed batch 491/785\n",
      "Processed batch 492/785\n",
      "Processed batch 493/785\n",
      "Processed batch 494/785\n",
      "Processed batch 495/785\n",
      "Processed batch 496/785\n",
      "Processed batch 497/785\n",
      "Processed batch 498/785\n",
      "Processed batch 499/785\n",
      "Processed batch 500/785\n",
      "Processed batch 501/785\n",
      "Processed batch 502/785\n",
      "Processed batch 503/785\n",
      "Processed batch 504/785\n",
      "Processed batch 505/785\n",
      "Processed batch 506/785\n",
      "Processed batch 507/785\n",
      "Processed batch 508/785\n",
      "Processed batch 509/785\n",
      "Processed batch 510/785\n",
      "Processed batch 511/785\n",
      "Processed batch 512/785\n",
      "Processed batch 513/785\n",
      "Processed batch 514/785\n",
      "Processed batch 515/785\n",
      "Processed batch 516/785\n",
      "Processed batch 517/785\n",
      "Processed batch 518/785\n",
      "Processed batch 519/785\n",
      "Processed batch 520/785\n",
      "Processed batch 521/785\n",
      "Processed batch 522/785\n",
      "Processed batch 523/785\n",
      "Processed batch 524/785\n",
      "Processed batch 525/785\n",
      "Processed batch 526/785\n",
      "Processed batch 527/785\n",
      "Processed batch 528/785\n",
      "Processed batch 529/785\n",
      "Processed batch 530/785\n",
      "Processed batch 531/785\n",
      "Processed batch 532/785\n",
      "Processed batch 533/785\n",
      "Processed batch 534/785\n",
      "Processed batch 535/785\n",
      "Processed batch 536/785\n",
      "Processed batch 537/785\n",
      "Processed batch 538/785\n",
      "Processed batch 539/785\n",
      "Processed batch 540/785\n",
      "Processed batch 541/785\n",
      "Processed batch 542/785\n",
      "Processed batch 543/785\n",
      "Processed batch 544/785\n",
      "Processed batch 545/785\n",
      "Processed batch 546/785\n",
      "Processed batch 547/785\n",
      "Processed batch 548/785\n",
      "Processed batch 549/785\n",
      "Processed batch 550/785\n",
      "Processed batch 551/785\n",
      "Processed batch 552/785\n",
      "Processed batch 553/785\n",
      "Processed batch 554/785\n",
      "Processed batch 555/785\n",
      "Processed batch 556/785\n",
      "Processed batch 557/785\n",
      "Processed batch 558/785\n",
      "Processed batch 559/785\n",
      "Processed batch 560/785\n",
      "Processed batch 561/785\n",
      "Processed batch 562/785\n",
      "Processed batch 563/785\n",
      "Processed batch 564/785\n",
      "Processed batch 565/785\n",
      "Processed batch 566/785\n",
      "Processed batch 567/785\n",
      "Processed batch 568/785\n",
      "Processed batch 569/785\n",
      "Processed batch 570/785\n",
      "Processed batch 571/785\n",
      "Processed batch 572/785\n",
      "Processed batch 573/785\n",
      "Processed batch 574/785\n",
      "Processed batch 575/785\n",
      "Processed batch 576/785\n",
      "Processed batch 577/785\n",
      "Processed batch 578/785\n",
      "Processed batch 579/785\n",
      "Processed batch 580/785\n",
      "Processed batch 581/785\n",
      "Processed batch 582/785\n",
      "Processed batch 583/785\n",
      "Processed batch 584/785\n",
      "Processed batch 585/785\n",
      "Processed batch 586/785\n",
      "Processed batch 587/785\n",
      "Processed batch 588/785\n",
      "Processed batch 589/785\n",
      "Processed batch 590/785\n",
      "Processed batch 591/785\n",
      "Processed batch 592/785\n",
      "Processed batch 593/785\n",
      "Processed batch 594/785\n",
      "Processed batch 595/785\n",
      "Processed batch 596/785\n",
      "Processed batch 597/785\n",
      "Processed batch 598/785\n",
      "Processed batch 599/785\n",
      "Processed batch 600/785\n",
      "Processed batch 601/785\n",
      "Processed batch 602/785\n",
      "Processed batch 603/785\n",
      "Processed batch 604/785\n",
      "Processed batch 605/785\n",
      "Processed batch 606/785\n",
      "Processed batch 607/785\n",
      "Processed batch 608/785\n",
      "Processed batch 609/785\n",
      "Processed batch 610/785\n",
      "Processed batch 611/785\n",
      "Processed batch 612/785\n",
      "Processed batch 613/785\n",
      "Processed batch 614/785\n",
      "Processed batch 615/785\n",
      "Processed batch 616/785\n",
      "Processed batch 617/785\n",
      "Processed batch 618/785\n",
      "Processed batch 619/785\n",
      "Processed batch 620/785\n",
      "Processed batch 621/785\n",
      "Processed batch 622/785\n",
      "Processed batch 623/785\n",
      "Processed batch 624/785\n",
      "Processed batch 625/785\n",
      "Processed batch 626/785\n",
      "Processed batch 627/785\n",
      "Processed batch 628/785\n",
      "Processed batch 629/785\n",
      "Processed batch 630/785\n",
      "Processed batch 631/785\n",
      "Processed batch 632/785\n",
      "Processed batch 633/785\n",
      "Processed batch 634/785\n",
      "Processed batch 635/785\n",
      "Processed batch 636/785\n",
      "Processed batch 637/785\n",
      "Processed batch 638/785\n",
      "Processed batch 639/785\n",
      "Processed batch 640/785\n",
      "Processed batch 641/785\n",
      "Processed batch 642/785\n",
      "Processed batch 643/785\n",
      "Processed batch 644/785\n",
      "Processed batch 645/785\n",
      "Processed batch 646/785\n",
      "Processed batch 647/785\n",
      "Processed batch 648/785\n",
      "Processed batch 649/785\n",
      "Processed batch 650/785\n",
      "Processed batch 651/785\n",
      "Processed batch 652/785\n",
      "Processed batch 653/785\n",
      "Processed batch 654/785\n",
      "Processed batch 655/785\n",
      "Processed batch 656/785\n",
      "Processed batch 657/785\n",
      "Processed batch 658/785\n",
      "Processed batch 659/785\n",
      "Processed batch 660/785\n",
      "Processed batch 661/785\n",
      "Processed batch 662/785\n",
      "Processed batch 663/785\n",
      "Processed batch 664/785\n",
      "Processed batch 665/785\n",
      "Processed batch 666/785\n",
      "Processed batch 667/785\n",
      "Processed batch 668/785\n",
      "Processed batch 669/785\n",
      "Processed batch 670/785\n",
      "Processed batch 671/785\n",
      "Processed batch 672/785\n",
      "Processed batch 673/785\n",
      "Processed batch 674/785\n",
      "Processed batch 675/785\n",
      "Processed batch 676/785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 677/785\n",
      "Processed batch 678/785\n",
      "Processed batch 679/785\n",
      "Processed batch 680/785\n",
      "Processed batch 681/785\n",
      "Processed batch 682/785\n",
      "Processed batch 683/785\n",
      "Processed batch 684/785\n",
      "Processed batch 685/785\n",
      "Processed batch 686/785\n",
      "Processed batch 687/785\n",
      "Processed batch 688/785\n",
      "Processed batch 689/785\n",
      "Processed batch 690/785\n",
      "Processed batch 691/785\n",
      "Processed batch 692/785\n",
      "Processed batch 693/785\n",
      "Processed batch 694/785\n",
      "Processed batch 695/785\n",
      "Processed batch 696/785\n",
      "Processed batch 697/785\n",
      "Processed batch 698/785\n",
      "Processed batch 699/785\n",
      "Processed batch 700/785\n",
      "Processed batch 701/785\n",
      "Processed batch 702/785\n",
      "Processed batch 703/785\n",
      "Processed batch 704/785\n",
      "Processed batch 705/785\n",
      "Processed batch 706/785\n",
      "Processed batch 707/785\n",
      "Processed batch 708/785\n",
      "Processed batch 709/785\n",
      "Processed batch 710/785\n",
      "Processed batch 711/785\n",
      "Processed batch 712/785\n",
      "Processed batch 713/785\n",
      "Processed batch 714/785\n",
      "Processed batch 715/785\n",
      "Processed batch 716/785\n",
      "Processed batch 717/785\n",
      "Processed batch 718/785\n",
      "Processed batch 719/785\n",
      "Processed batch 720/785\n",
      "Processed batch 721/785\n",
      "Processed batch 722/785\n",
      "Processed batch 723/785\n",
      "Processed batch 724/785\n",
      "Processed batch 725/785\n",
      "Processed batch 726/785\n",
      "Processed batch 727/785\n",
      "Processed batch 728/785\n",
      "Processed batch 729/785\n",
      "Processed batch 730/785\n",
      "Processed batch 731/785\n",
      "Processed batch 732/785\n",
      "Processed batch 733/785\n",
      "Processed batch 734/785\n",
      "Processed batch 735/785\n",
      "Processed batch 736/785\n",
      "Processed batch 737/785\n",
      "Processed batch 738/785\n",
      "Processed batch 739/785\n",
      "Processed batch 740/785\n",
      "Processed batch 741/785\n",
      "Processed batch 742/785\n",
      "Processed batch 743/785\n",
      "Processed batch 744/785\n",
      "Processed batch 745/785\n",
      "Processed batch 746/785\n",
      "Processed batch 747/785\n",
      "Processed batch 748/785\n",
      "Processed batch 749/785\n",
      "Processed batch 750/785\n",
      "Processed batch 751/785\n",
      "Processed batch 752/785\n",
      "Processed batch 753/785\n",
      "Processed batch 754/785\n",
      "Processed batch 755/785\n",
      "Processed batch 756/785\n",
      "Processed batch 757/785\n",
      "Processed batch 758/785\n",
      "Processed batch 759/785\n",
      "Processed batch 760/785\n",
      "Processed batch 761/785\n",
      "Processed batch 762/785\n",
      "Processed batch 763/785\n",
      "Processed batch 764/785\n",
      "Processed batch 765/785\n",
      "Processed batch 766/785\n",
      "Processed batch 767/785\n",
      "Processed batch 768/785\n",
      "Processed batch 769/785\n",
      "Processed batch 770/785\n",
      "Processed batch 771/785\n",
      "Processed batch 772/785\n",
      "Processed batch 773/785\n",
      "Processed batch 774/785\n",
      "Processed batch 775/785\n",
      "Processed batch 776/785\n",
      "Processed batch 777/785\n",
      "Processed batch 778/785\n",
      "Processed batch 779/785\n",
      "Processed batch 780/785\n",
      "Processed batch 781/785\n",
      "Processed batch 782/785\n",
      "Processed batch 783/785\n",
      "Processed batch 784/785\n",
      "Processed batch 785/785\n",
      "Final dataset: 39209 samples with 57 features each\n",
      "Training models using class labels...\n",
      "Training Random Forest...\n",
      "Random Forest validation score: 0.9476\n",
      "Best model: Random Forest with score: 0.9476\n",
      "Best model (Random Forest) saved to .\\models\\best_model.joblib\n",
      "Evaluating model...\n",
      "Generating reports and visualizations...\n",
      "Evaluation complete. Results saved to .\\results\n",
      "Overall accuracy: 0.9476\n",
      "\n",
      "Class distribution:\n",
      "Class 0: 210 images (0.54%)\n",
      "Class 1: 2220 images (5.66%)\n",
      "Class 10: 2010 images (5.13%)\n",
      "Class 11: 1320 images (3.37%)\n",
      "Class 12: 2100 images (5.36%)\n",
      "Class 13: 2160 images (5.51%)\n",
      "Class 14: 780 images (1.99%)\n",
      "Class 15: 630 images (1.61%)\n",
      "Class 16: 420 images (1.07%)\n",
      "Class 17: 1110 images (2.83%)\n",
      "Class 18: 1200 images (3.06%)\n",
      "Class 19: 210 images (0.54%)\n",
      "Class 2: 2250 images (5.74%)\n",
      "Class 20: 360 images (0.92%)\n",
      "Class 21: 330 images (0.84%)\n",
      "Class 22: 390 images (0.99%)\n",
      "Class 23: 510 images (1.30%)\n",
      "Class 24: 270 images (0.69%)\n",
      "Class 25: 1500 images (3.83%)\n",
      "Class 26: 600 images (1.53%)\n",
      "Class 27: 240 images (0.61%)\n",
      "Class 28: 540 images (1.38%)\n",
      "Class 29: 270 images (0.69%)\n",
      "Class 3: 1410 images (3.60%)\n",
      "Class 30: 450 images (1.15%)\n",
      "Class 31: 780 images (1.99%)\n",
      "Class 32: 240 images (0.61%)\n",
      "Class 33: 689 images (1.76%)\n",
      "Class 34: 420 images (1.07%)\n",
      "Class 35: 1200 images (3.06%)\n",
      "Class 36: 390 images (0.99%)\n",
      "Class 37: 210 images (0.54%)\n",
      "Class 38: 2070 images (5.28%)\n",
      "Class 39: 300 images (0.77%)\n",
      "Class 4: 1980 images (5.05%)\n",
      "Class 40: 360 images (0.92%)\n",
      "Class 41: 240 images (0.61%)\n",
      "Class 42: 240 images (0.61%)\n",
      "Class 5: 1860 images (4.74%)\n",
      "Class 6: 420 images (1.07%)\n",
      "Class 7: 1440 images (3.67%)\n",
      "Class 8: 1410 images (3.60%)\n",
      "Class 9: 1470 images (3.75%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_image(image_path, target_size=(64, 64)):\n",
    "    \"\"\"Load and resize image to standard size to reduce memory usage\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize(target_size)\n",
    "        return np.array(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def rgb_to_hsv(rgb_img):\n",
    "    \"\"\"Convert RGB to HSV using cv2 which is more efficient\"\"\"\n",
    "    return cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "def preprocess_batch(image_paths, target_size=(64, 64)):\n",
    "    \"\"\"Process a batch of images at once to improve efficiency\"\"\"\n",
    "    processed_images = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    for path in image_paths:\n",
    "        img = load_image(path, target_size)\n",
    "        if img is not None:\n",
    "            processed_images.append(img)\n",
    "            valid_paths.append(path)\n",
    "    \n",
    "    return processed_images, valid_paths\n",
    "\n",
    "# ===============   FEATURE EXTRACTION   ===============\n",
    "\n",
    "def extract_features_batch(images):\n",
    "    \"\"\"Extract features from a batch of images\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Convert to HSV for color features\n",
    "        hsv_img = rgb_to_hsv(img)\n",
    "        \n",
    "        # Color features - histogram of hue and saturation channels\n",
    "        h_hist = cv2.calcHist([hsv_img], [0], None, [16], [0, 180])\n",
    "        s_hist = cv2.calcHist([hsv_img], [1], None, [16], [0, 256])\n",
    "        v_hist = cv2.calcHist([hsv_img], [2], None, [16], [0, 256])\n",
    "        \n",
    "        # Shape features using HOG (Histogram of Oriented Gradients)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Calculate image statistics\n",
    "        mean_color = np.mean(img, axis=(0, 1))\n",
    "        std_color = np.std(img, axis=(0, 1))\n",
    "        \n",
    "        # Simple edge detection for shape features\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])\n",
    "        \n",
    "        # Basic shape descriptors\n",
    "        try:\n",
    "            # Convert grayscale to binary\n",
    "            _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                # Get the largest contour\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                area = cv2.contourArea(largest_contour)\n",
    "                perimeter = cv2.arcLength(largest_contour, True)\n",
    "                \n",
    "                # Calculate circularity\n",
    "                circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
    "                \n",
    "                # Calculate aspect ratio using minimum area rectangle\n",
    "                rect = cv2.minAreaRect(largest_contour)\n",
    "                width, height = rect[1]\n",
    "                aspect_ratio = width / height if height > 0 else 0\n",
    "                if aspect_ratio < 1:\n",
    "                    aspect_ratio = 1 / aspect_ratio\n",
    "            else:\n",
    "                circularity = 0\n",
    "                aspect_ratio = 1\n",
    "        except:\n",
    "            circularity = 0\n",
    "            aspect_ratio = 1\n",
    "        \n",
    "        # Collect all features\n",
    "        feature_vector = np.concatenate([\n",
    "            h_hist.flatten() / np.sum(h_hist) if np.sum(h_hist) > 0 else h_hist.flatten(),  # Normalized hue histogram\n",
    "            s_hist.flatten() / np.sum(s_hist) if np.sum(s_hist) > 0 else s_hist.flatten(),  # Normalized saturation histogram\n",
    "            v_hist.flatten() / np.sum(v_hist) if np.sum(v_hist) > 0 else v_hist.flatten(),  # Normalized value histogram\n",
    "            mean_color,  # Mean color (3 values)\n",
    "            std_color,   # Standard deviation of color (3 values)\n",
    "            [edge_density, circularity, aspect_ratio]  # Shape features\n",
    "        ])\n",
    "        \n",
    "        features_list.append(feature_vector)\n",
    "    \n",
    "    return np.array(features_list)\n",
    "\n",
    "# =============== CLUSTERING FOR UNSUPERVISED LEARNING ===============\n",
    "\n",
    "def perform_clustering(X, n_clusters=6):\n",
    "    \"\"\"Perform K-means clustering to group similar images\"\"\"\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Calculate silhouette score if sklearn.metrics is available\n",
    "    try:\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return kmeans, cluster_labels\n",
    "\n",
    "# =============== TRAINING AND EVALUATION ===============\n",
    "\n",
    "def train_models(X_train, y_train, X_val=None, y_val=None):\n",
    "    \"\"\"Train multiple models and select the best one\"\"\"\n",
    "    models = {}\n",
    "    best_score = 0\n",
    "    best_model_name = None\n",
    "    \n",
    "    # 1. Random Forest Classifier\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_score = rf.score(X_val, y_val) if X_val is not None else rf.score(X_train, y_train)\n",
    "    models[\"Random Forest\"] = rf\n",
    "    \n",
    "    print(f\"Random Forest validation score: {rf_score:.4f}\")\n",
    "    \n",
    "    if rf_score > best_score:\n",
    "        best_score = rf_score\n",
    "        best_model_name = \"Random Forest\"\n",
    "    \n",
    "    # 2. SVM with RBF kernel (if dataset is not too large)\n",
    "    if X_train.shape[0] < 5000:  # Only train SVM if dataset is manageable\n",
    "        print(\"Training SVM...\")\n",
    "        svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "        svm.fit(X_train, y_train)\n",
    "        svm_score = svm.score(X_val, y_val) if X_val is not None else svm.score(X_train, y_train)\n",
    "        models[\"SVM\"] = svm\n",
    "        \n",
    "        print(f\"SVM validation score: {svm_score:.4f}\")\n",
    "        \n",
    "        if svm_score > best_score:\n",
    "            best_score = svm_score\n",
    "            best_model_name = \"SVM\"\n",
    "    \n",
    "    print(f\"Best model: {best_model_name} with score: {best_score:.4f}\")\n",
    "    return models, best_model_name\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, class_names):\n",
    "    \"\"\"Evaluate model performance and generate reports\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, cm, y_pred\n",
    "\n",
    "# =============== VISUALIZATION FUNCTIONS ===============\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, output_file):\n",
    "    \"\"\"Plot and save confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, output_file):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[-20:]  # Get top 20 features\n",
    "        \n",
    "        plt.title('Feature Importances')\n",
    "        plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] if i < len(feature_names) else f\"Feature {i}\" for i in indices])\n",
    "        plt.xlabel('Relative Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "\n",
    "def plot_clusters(X, cluster_labels, output_file, method='pca'):\n",
    "    \"\"\"Visualize clusters in 2D using PCA or t-SNE\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Use PCA or t-SNE for dimensionality reduction\n",
    "    if method == 'pca':\n",
    "        from sklearn.decomposition import PCA\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    else:  # t-SNE\n",
    "        from sklearn.manifold import TSNE\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "    \n",
    "    # Reduce dimensions to 2D\n",
    "    X_2d = reducer.fit_transform(X)\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    plt.title(f'Image Clusters Visualization using {method.upper()}')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "\n",
    "def plot_class_samples(images, labels, class_labels, output_dir, n_samples=5):\n",
    "    \"\"\"Save sample images from each class\"\"\"\n",
    "    unique_classes = np.unique(labels)\n",
    "    \n",
    "    for class_id in unique_classes:\n",
    "        # Get indices of images in this class\n",
    "        class_indices = np.where(labels == class_id)[0]\n",
    "        \n",
    "        # Skip if no images in class\n",
    "        if len(class_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Select sample images (up to n_samples)\n",
    "        sample_indices = class_indices[:min(n_samples, len(class_indices))]\n",
    "        \n",
    "        # Create a grid to display samples\n",
    "        rows = 1\n",
    "        cols = len(sample_indices)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "        \n",
    "        # Make axes iterable if there's only one sample\n",
    "        if cols == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Plot each sample\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            axes[i].imshow(images[idx])\n",
    "            axes[i].set_title(f'Class {class_labels[class_id]}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'class_{class_labels[class_id]}_samples.png'))\n",
    "        plt.close()\n",
    "\n",
    "def write_reports(accuracy, classification_report_dict, output_file):\n",
    "    \"\"\"Write detailed performance reports to a file\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Overall Accuracy: {accuracy:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        \n",
    "        # Write per-class metrics\n",
    "        for class_name, metrics in classification_report_dict.items():\n",
    "            if class_name in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                continue\n",
    "                \n",
    "            f.write(f\"\\nClass: {class_name}\\n\")\n",
    "            f.write(f\"  Precision: {metrics['precision']:.4f}\\n\")\n",
    "            f.write(f\"  Recall: {metrics['recall']:.4f}\\n\")\n",
    "            f.write(f\"  F1-score: {metrics['f1-score']:.4f}\\n\")\n",
    "            f.write(f\"  Support: {metrics['support']}\\n\")\n",
    "        \n",
    "        # Write summary metrics\n",
    "        f.write(\"\\nSummary:\\n\")\n",
    "        for avg_type in ['macro avg', 'weighted avg']:\n",
    "            if avg_type in classification_report_dict:\n",
    "                f.write(f\"{avg_type}:\\n\")\n",
    "                metrics = classification_report_dict[avg_type]\n",
    "                f.write(f\"  Precision: {metrics['precision']:.4f}\\n\")\n",
    "                f.write(f\"  Recall: {metrics['recall']:.4f}\\n\")\n",
    "                f.write(f\"  F1-score: {metrics['f1-score']:.4f}\\n\")\n",
    "                f.write(f\"  Support: {metrics['support']}\\n\")\n",
    "\n",
    "# =============== MAIN EXECUTION ===============\n",
    "\n",
    "def main():\n",
    "    # --- Project Setup ---\n",
    "    PROJECT_ROOT = \".\"\n",
    "    RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\n",
    "    MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "    TRAIN_DIR = os.path.join(PROJECT_ROOT, \"Train\")  # Update this to your Train directory\n",
    "    \n",
    "    # Create necessary directories\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.join(RESULTS_DIR, \"classes\"), exist_ok=True)\n",
    "    \n",
    "    # Define output files\n",
    "    CONFUSION_MATRIX_FILE = os.path.join(RESULTS_DIR, \"confusion_matrix.png\")\n",
    "    FEATURE_IMPORTANCE_FILE = os.path.join(RESULTS_DIR, \"feature_importance.png\")\n",
    "    METRICS_FILE = os.path.join(RESULTS_DIR, \"metrics.txt\")\n",
    "    MODEL_FILE = os.path.join(MODELS_DIR, \"best_model.joblib\")\n",
    "    \n",
    "    print(\"Finding image files...\")\n",
    "    \n",
    "    # Scan for class folders in the Train directory\n",
    "    if not os.path.exists(TRAIN_DIR):\n",
    "        print(f\"Training directory {TRAIN_DIR} does not exist. Please check the path.\")\n",
    "        return\n",
    "    \n",
    "    class_folders = [folder for folder in os.listdir(TRAIN_DIR) \n",
    "                     if os.path.isdir(os.path.join(TRAIN_DIR, folder))]\n",
    "    \n",
    "    if not class_folders:\n",
    "        print(f\"No class folders found in {TRAIN_DIR}. Please check the directory structure.\")\n",
    "        return\n",
    "    \n",
    "    # Sort class folders and create class mapping\n",
    "    class_folders.sort()  # Ensure consistent ordering\n",
    "    class_to_index = {class_name: i for i, class_name in enumerate(class_folders)}\n",
    "    index_to_class = {i: class_name for class_name, i in class_to_index.items()}\n",
    "    \n",
    "    print(f\"Found {len(class_folders)} classes: {', '.join(class_folders)}\")\n",
    "    \n",
    "    # Collect image paths and labels\n",
    "    supported_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    image_files = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in class_folders:\n",
    "        class_dir = os.path.join(TRAIN_DIR, class_name)\n",
    "        class_idx = class_to_index[class_name]\n",
    "        \n",
    "        for file in os.listdir(class_dir):\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            if ext in supported_extensions:\n",
    "                image_files.append(os.path.join(class_dir, file))\n",
    "                labels.append(class_idx)\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files across {len(class_folders)} classes\")\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(f\"No image files found in {TRAIN_DIR}. Please check the directory structure.\")\n",
    "        return\n",
    "    \n",
    "    # Process images in batches to avoid memory issues\n",
    "    print(\"Processing images and extracting features...\")\n",
    "    batch_size = 50\n",
    "    all_features = []\n",
    "    all_images = []\n",
    "    valid_paths = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for i in range(0, len(image_files), batch_size):\n",
    "        batch_paths = image_files[i:i+batch_size]\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "        \n",
    "        # Process batch\n",
    "        processed_images, valid_batch_paths = preprocess_batch(batch_paths)\n",
    "        \n",
    "        if processed_images:\n",
    "            # Extract features\n",
    "            batch_features = extract_features_batch(processed_images)\n",
    "            \n",
    "            all_features.append(batch_features)\n",
    "            all_images.extend(processed_images)\n",
    "            valid_paths.extend(valid_batch_paths)\n",
    "            \n",
    "            # Keep only the labels for valid images\n",
    "            for j, path in enumerate(batch_paths):\n",
    "                if path in valid_batch_paths:\n",
    "                    valid_labels.append(batch_labels[j])\n",
    "            \n",
    "        print(f\"Processed batch {i//batch_size + 1}/{len(image_files)//batch_size + 1}\")\n",
    "    \n",
    "    # Combine all batches\n",
    "    X = np.vstack(all_features) if all_features else np.array([])\n",
    "    y = np.array(valid_labels)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No valid images processed. Please check your image paths and data.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Final dataset: {X.shape[0]} samples with {X.shape[1]} features each\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create feature names for visualization\n",
    "    feature_names = (\n",
    "        [f\"hue_bin_{i}\" for i in range(16)] +\n",
    "        [f\"sat_bin_{i}\" for i in range(16)] +\n",
    "        [f\"val_bin_{i}\" for i in range(16)] +\n",
    "        [\"mean_r\", \"mean_g\", \"mean_b\"] + \n",
    "        [\"std_r\", \"std_g\", \"std_b\"] +\n",
    "        [\"edge_density\", \"circularity\", \"aspect_ratio\"]\n",
    "    )\n",
    "    \n",
    "    # Split data for supervised learning\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    print(\"Training models using class labels...\")\n",
    "    models, best_model_name = train_models(X_train, y_train, X_test, y_test)\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    # Save the best model\n",
    "    dump(best_model, MODEL_FILE)\n",
    "    print(f\"Best model ({best_model_name}) saved to {MODEL_FILE}\")\n",
    "    \n",
    "    # Evaluate multi-class model\n",
    "    print(\"Evaluating model...\")\n",
    "    class_names = [index_to_class[i] for i in range(len(class_folders))]\n",
    "    accuracy, report, cm, y_pred = evaluate_model(best_model, X_test, y_test, class_names)\n",
    "    \n",
    "    # Generate visualizations and reports\n",
    "    print(\"Generating reports and visualizations...\")\n",
    "    plot_confusion_matrix(cm, class_names, CONFUSION_MATRIX_FILE)\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        plot_feature_importance(best_model, feature_names, FEATURE_IMPORTANCE_FILE)\n",
    "    write_reports(accuracy, report, METRICS_FILE)\n",
    "    \n",
    "    # Visualize sample images from each class\n",
    "    plot_class_samples(all_images, y, index_to_class, os.path.join(RESULTS_DIR, \"classes\"))\n",
    "    \n",
    "    print(f\"Evaluation complete. Results saved to {RESULTS_DIR}\")\n",
    "    print(f\"Overall accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for class_idx, class_name in index_to_class.items():\n",
    "        count = np.sum(y == class_idx)\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"Class {class_name}: {count} images ({percentage:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18f5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
